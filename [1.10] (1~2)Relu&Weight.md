# Relu
relu를 왜쓰는가

#### Sigmoid의 한계
deep한 네트워크에서 sigmoid의 기울기가 0에 가까워지는 부분이 여러개 겹치게 되면 backpropagation으로 전달되는 값이 거의 없어진다는 것입니다.

#### Relu는 무엇인가
0보다 작은수는0 나머지는 그대로. max(0,x)  
activation function중 하나이다.  
relu는 sigmoid의 위와같은 문제를 방지할 수 있습니다. 하니만 relu또한 한계가 있는데 0이하인 데이터를 모두 0 으로 바꾸기 때문에 0이하 값에 대한 경계가 없어집니다.

# Weight Initialization
Weight 초기점 설정이 중요한 이유는 다차원의 Weight 와 Cost그래프를 보면 쉽게 이해할 수 있는데,  
그림에서 Cost를 줄여나가는데 출발점에 따라 Local minimum에 빠질 확률이 달라진다는것을 쉽게 알 수 있었다.  
이전 강좌에서 평균은 0 분산은 1로된 초기화를 사용했을 것 입니다.  
Xavier Initialization은 input output의 체널의 갯수를 이용한 식을 도입하여 분산을 조정하는 방식입니다.  
He Initialization은 ReLU에 특화되어 있는 초기화 방법이라고 합니다.  

