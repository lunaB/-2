# Dropout

#### 시작
underfitting된 신경망은 train데이터와 test데이터를 모두 못맞춘다.  
잘학습된 신경망은 train도 적당히 맞추고 test데이터도 잘맞춘다.  
overfitting된 신경망은 train을 매우 잘맞추지만 test데이터를 잘 맞추지 못한다.

dropout은 이런 underfitting과 overfitting문제를 해결하기에 좋다.  

#### Dropout이 무엇인가
노드중 일부를 비활성화해 학습을 시키는 방법이다.  
어떤 노드를 비활성화 하는지는 랜덤으로 정해진다.  
학습후에는 모든 노드를 활성화해 feed를 한다.  

이와 같은 기법이 유용한이유는 input데이터의 특정부분을 가지고만 특정노드가 target에 가까운 결과를 만들 수 있는 힘을 갖게 한다는것입니다.  
이를정리하면 고양이의 전체 모습을보고 모든노드가 같이 학습하기보다 각노드가 일부분을 보고 자세히 학습할 수 있기를 바라며 만든것이다 정도 할 수 있겠습니다.  

test할때는 training false로 하고 사용하면 되겠습니다.

# Batch
Internal Covariate Shift 영상이 이해가 안되서 따로 찾아봤는데,  
학습을 하며 layer가 이동할수록 weight가 적용되어 레이어들이 갖는 output값들의 분포가 변형되는데,  
이러한 변형을 줄이면 학습의 속도를 높일 수 있다는 내용이였습니다.

## 참고
Dropout은 이런식으로 사용하고 
```
relu()
dropout()
```
Batch_norm은 이렇게 많이 쓴다고합니. 순서가 다르죠?  
사실 위치가 바뀌어도 돌아가며 일부러 그렇게 쓰기도 한다고 합니다.
```
batch_norm()
relu()
```