# Application and tips
현업에서 사용하는 기술들과 팁들을 설명하는 챕터 입니다.

## Learning rate
learning rate는 경사하강법에서 cost를 줄이며 그래프를 따라 내려가는 거리를 정하는것입니다.  
이해를 돕기위하여 쉽게 설명하자면 
`Overshooting`은 learning rate가 너무커서 Cost그래프의 아래쪽으로 내려가지 않고, 그래프의 반대편으로 이동하는 등의 문제 즉 Cost의 최소 지점을 지나쳐버려 Cost가 증가하는것을 말한다.

#### Learning rate decay
train이 진행됨에 따라 cost값은 감소하여 Cost그래프의 Cost의 최소지점에 가까워 지는데 이때 `Overshooting`처럼 Cost의 최소 지점을 지나침을 반복하며 학습에 진전이 생기지 않는 경우가 있는데 이때 cost를 더 낮추기 위하여 학습이 진행됨에 따라 `learning rate`를 낮춰 주는 기법이다.

## Data preprocessing
데이터 전처리

#### Standardization 표준화
평균에서 얼마만큼 떨어졌나를 나타냄 (표준정규분포를 따르는 표준화식을 사용함.)

#### Normalization 정규화
(0~1)의 사이로 데이터를 분포하는것입니다. 데이터의 최대값을 1로 최소값을 0으로 맞추어 값들을 변환

#### Noisy Data
학습의 의도에 맞지않는 극단적인값을 갖거나 혹은 쓸모없는 데이터 혹은 처리되지 않은 데이터들을 뜻합니다.  
이러한 데이터들을 버림으로서 학습의 정확도를 높일수 있습니다.  


## Overfitting 과적합
우리는 학습을 시킬때 모든 데이터들을 대표하는 모델을 만들어야 하고 그것을 만들기위해 작은 데이터셋을 부여받습니다.  
하지만 그 작은 데이터셋에 과적합하게 학습되면 모든 데이터에서는 낮은 정확도를 보이고 학습한 데이터에만 높은 정확도를 갖는 모델이 만들어집니다. 이를 `Overfitting`이라 부릅니다.  
반대로 학습이 적합하게 되지 않은것을 `Underfit`하다고 합니다.

## Smaller set of features
필요없는 차원(features)을 지워서 차원을 낮추어 정확도를 높힙니다. (PCA 라는방법을 많이 쓰고 sklearn에서 라이브러리로 제공하고있다고 합니다.)

## Add additional features
모델이 너무 간단할경우 구체화 시킬 필요가 잆습니다. `underfitting`이 일어났을때 의미를 갖는 feature을 추가함으로 정확도를 높일수 있습니다.  
이전의 `Smaller set of features`와 상반됩니다.

## Regularization (Add term to loss)
`Underfitting`값과 `Overfitting`값을 이용하여 정규화의 효과를 볼수 있다.  
그래프를 동반한 설명이 필요하여 텍스트로 정리하기는 힘들 것 같다.

## Solutions
- Feature Normalization
- Regularization
- 데이터 추가
    - 학습할 데이터를 임의로 추가하는 방법이다. Image데이터 같은경우에는 색을 여러색들로 바꾸어 학습시킬 수 도 있고, 뒤집을 수 도 있고, 자르기도 하여 만들 수 있다.
- 9강에서 설명
    - Dropout
    - Batch Normalization