# Gradient descent
경사하강법으로도 불리는 이는 코스트함수가 최소가 되는 W 와 b 값을 알아내기 위하여 만들어 진 것이다.  
W를 인수로 갖는 코스트 함수 그래프의 현제 미분값에 learning rate와 마이너스를 곱해준값을 기존의 값에 더해주며 코스트를 줄여나가는 방식이다.  

#### 문제점 / 경고
경사하강법을 사용하다보면 cost값을 감소시키는 다양한 경우가 생기게 됩니다. 초기값 혹은 다른 요인에 의하여 똑같이 학습시켰어도 다른 값으로 cost 최적화가 이루어 질수 있다는 것입니다.  
주변 W b cost 3차원 그래프에서 주변에 비하여 낮은 값을 갖는 부분을 local minimum 이라고 하며,  
여럿 존재 할수 있는데, 이는 같은 cost값을 갖더라도 W나 b 값이 다를수 있다는것을 말하고,  
높은 cost에서부터 W와 b의 값을 바꾸면서 내려온 local minimum이 모든 local minimum중 가장 낮은 값이라고 보장할수 없는 문제가 존재합니다.